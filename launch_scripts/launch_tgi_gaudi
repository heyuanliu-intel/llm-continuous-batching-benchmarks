#!/usr/bin/env bash
# set -ex

if [ -z ${PORT+x} ]; then
    echo "missing PORT"
    exit 1
fi

if [ -z ${model_name+x} ]; then
    echo "missing model_name"
    exit 1
fi

if [ -z ${max_batch_total_tokens+x} ]; then
    echo "missing max_batch_total_tokens"
    exit 1
fi

# export ENABLE_EXPERIMENTAL_FLAGS=1
# export VISUALIZATION_MODE=0
# export GRAPH_VISUALIZATION=1

# TEXT_GENERATION_SERVER_IGNORE_EOS_TOKEN=true \
# text-generation-launcher \
#     --model-id $model_name \
#     --sharded false  \
#     --huggingface-hub-cache /data/cache \
#     --max-input-length 1024 \
#     --max-total-tokens 2048 \
#     --max-batch-total-tokens $max_batch_total_tokens \
#     --max-waiting-tokens 7 \
#     --waiting-served-ratio 1.2 \
#     --port $PORT \
#     --shard-uds-path /tmp/text-generation-server.2 \
#     --max-concurrent-requests 2000

if [ "$num_shard" -ne 1 ]; then
    echo "used $num_shard  card for inference "
    export PT_HPU_ENABLE_LAZY_COLLECTIVES=true
    shard_flag=true
    echo "shard_flag is $shard_flag"
else
    echo "used single card for inference"
    shard_flag=false
    echo "shard_flag is $shard_flag"
fi

TRUST_REMOTE_CODE=true TEXT_GENERATION_SERVER_IGNORE_EOS_TOKEN=true \
text-generation-launcher \
    --model-id $model_name \
    --sharded $shard_flag  \
    --huggingface-hub-cache /data/cache \
    --max-input-length ${max_input} \
    --max-total-tokens ${max_total} \
    --max-batch-total-tokens ${max_batch_total_tokens} \
    --max-waiting-tokens 20 \
    --max-batch-prefill-tokens 2048 \
    --waiting-served-ratio 1.2 \
    --port 80 \
    --shard-uds-path /tmp/text-generation-server.2 \
    --max-concurrent-requests $bs \
    --num-shard $num_shard

